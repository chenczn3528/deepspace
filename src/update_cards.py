# -*- coding: utf-8 -*-
from __future__ import annotations
from pathlib import Path

"""
æ‹ä¸æ·±ç©º WIKI æŠ½å¡å¡ç‰‡çˆ¬è™«
----------------------------------------------------------------
- ä»…ä¿å­˜å›¾ç‰‡/è§†é¢‘é“¾æ¥ï¼Œä¸ä¸‹è½½æ–‡ä»¶
- ç¤¼è²Œ UAã€é™é€Ÿã€Session+Retry
- å…¼å®¹ iframe è¢«è½¬ä¹‰æˆæ–‡æœ¬ï¼ˆ&lt;iframe ...&gt;ï¼‰
- B ç«™æ’­æ”¾å™¨ URL è‡ªåŠ¨è¡¥å‚æ•°å¹¶å°è¯•åŒ¹é…åˆ† P
"""

import json
import time
import random
import re
import html
from copy import deepcopy
import requests
import mwclient
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# -----------------------------
# é…ç½®
# -----------------------------
CARDS_PATH = "src/assets/cards.json"
POOL_CATEGORIES_PATH = "src/assets/poolCategories.json"
WIKI_BASE = "https://wiki.biligame.com/lysk/"
ENTRY_URL = WIKI_BASE + "%E6%80%9D%E5%BF%B5:%E7%AD%9B%E9%80%89"

# ç»Ÿä¸€è®¸å¯
LICENSE_NAME = "CC BY-NC-SA 4.0"
LICENSE_URL = "https://creativecommons.org/licenses/by-nc-sa/4.0/"

UA = "GachaSimCrawler/1.0 (+mailto:chenczn3528@gmail.com)"  # åˆè§„ UA
HEADERS = {"User-Agent": UA}

CATEGORY_PRIORITY = {
    ("wishSeries", "limited"): 1,
    ("wishSeries", "permanent"): 0,
}

CJK_RANGE = "\u4e00-\u9fff"
RE_WISH_AFTER = re.compile(rf"(?<![{CJK_RANGE}])è®¸æ„¿/")
RE_WISH_BEFORE = re.compile(rf"/è®¸æ„¿(?![{CJK_RANGE}])")
QUOTE_NAME_RE = re.compile(r"ã€Œ([^ã€]+)ã€")

DEFAULT_POOL_CATEGORIES = {
    "wishSeries": {
        "name": "è®¸æ„¿ç³»åˆ—",
        "icon": "ğŸŒ ",
        "subcategories": {
            "limited": {"name": "é™æ—¶å¡æ± ", "pools": []},
            "permanent": {"name": "å¸¸é©»å¡æ± ", "pools": []},
        },
    },
    "specialRewards": {
        "name": "å¯†çº¦/æŒšç¤¼ç³»åˆ—",
        "icon": "ğŸ",
        "pools": [],
    },
}

# -----------------------------
# ä¼šè¯ä¸ç¤¼è²Œè®¿é—®
# -----------------------------
session = requests.Session()
retries = Retry(
    total=5,
    backoff_factor=1.4,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods=["GET"],
    raise_on_status=False,
)
session.mount("https://", HTTPAdapter(max_retries=retries))
session.headers.update(HEADERS)

def polite_get(url: str, timeout: int = 15) -> requests.Response:
    """å¸¦éšæœºå»¶è¿Ÿçš„ GETï¼Œé¿å…ç»™ç«™ç‚¹é€ æˆå‹åŠ›ã€‚"""
    time.sleep(1.5 + random.random())  # 1.5~2.5s
    resp = session.get(url, timeout=timeout)
    resp.raise_for_status()
    if not resp.encoding:
        resp.encoding = resp.apparent_encoding or "utf-8"
    return resp

# -----------------------------
# å·¥å…·å‡½æ•°
# -----------------------------
def parse_best_from_srcset(srcset: str) -> str:
    """ä» srcset é€‰æ‹©æ¸…æ™°åº¦è¾ƒå¥½çš„ URLï¼šä¼˜å…ˆ 2xï¼Œæ²¡æœ‰å°±æœ€åä¸€é¡¹ã€‚"""
    if not srcset:
        return ""
    parts = [p.strip() for p in srcset.split(",") if p.strip()]
    if not parts:
        return ""
    for part in reversed(parts):
        if part.endswith(" 2x") or part.endswith("2x"):
            return part.rsplit(" ", 1)[0]
    return parts[-1].rsplit(" ", 1)[0] if " " in parts[-1] else parts[-1]

# â€”â€” ä»åŸå§‹ HTML ä¸­ç›´æ¥æŠ“ <iframe> å—ï¼›å¹¶å…¼å®¹è¢«è½¬ä¹‰çš„ HTML
IFRAME_BLOCK_RE = re.compile(r'<iframe.*?</iframe>', re.IGNORECASE | re.DOTALL)
IFRAME_SRC_RE   = re.compile(r'src=["\']([^"\']+)["\']', re.IGNORECASE)
WIKI_LINK_RE = re.compile(r"\[\[([^\]]+)\]\]")
FULL_WIDTH_BRACKET_RE = re.compile(r"ã€([^ã€‘]+)ã€‘")
QUOTE_NAME_RE = re.compile(r"ã€Œ([^ã€]+)ã€")
EVENT_GAIN_RE = re.compile(r"^åœ¨?(.+?)æ´»åŠ¨ä¸­è·å–$")

def extract_iframes(html_bytes_or_text) -> list[str]:
    """
    ä»åŸå§‹ HTMLï¼ˆbytes æˆ– strï¼‰æå– <iframe> å—ã€‚
    1) ç›´æ¥æ­£åˆ™
    2) è‹¥é¡µé¢æ˜¯è½¬ä¹‰æ–‡æœ¬ï¼ˆ&lt;iframe ...&gt;ï¼‰ï¼Œå…ˆ html.unescape å†æ­£åˆ™
    """
    if isinstance(html_bytes_or_text, bytes):
        raw = html_bytes_or_text.decode("utf-8", errors="ignore")
    else:
        raw = html_bytes_or_text or ""

    blocks = IFRAME_BLOCK_RE.findall(raw)
    if blocks:
        return blocks

    unescaped = html.unescape(raw)
    if unescaped != raw:
        blocks = IFRAME_BLOCK_RE.findall(unescaped)
        if blocks:
            return blocks

    return []


def normalize_event_pool_name(name: str) -> str:
    match = EVENT_GAIN_RE.match(name)
    if match:
        return match.group(1).strip()
    return name


def clean_pool_name(text: str) -> str:
    if not text:
        return ""
    cleaned = text.strip()
    if "|" in cleaned:
        cleaned = cleaned.split("|")[-1]
    for ch in ("[", "]", "ã€", "ã€‘"):
        cleaned = cleaned.replace(ch, "")
    cleaned = cleaned.replace("ã€Œ", "").replace("ã€", "")
    cleaned = normalize_event_pool_name(cleaned.strip())
    return cleaned


def extract_pool_name(get_str: str) -> str:
    if not get_str:
        return ""
    for pattern in (WIKI_LINK_RE, FULL_WIDTH_BRACKET_RE, QUOTE_NAME_RE):
        match = pattern.search(get_str)
        if match:
            candidate = clean_pool_name(match.group(1))
            if candidate:
                return candidate
    candidate = get_str.split("/")[0]
    candidate = candidate.split("ï¼Œ")[0]
    return clean_pool_name(candidate)


def extract_first_quote_name(get_str: str) -> str:
    if not get_str:
        return ""
    match = QUOTE_NAME_RE.search(get_str)
    if match:
        return clean_pool_name(match.group(1))
    return ""


def get_raw_get_field(card: dict) -> str:
    return (card.get("get") or "").replace("<br>", " ")


def has_standalone_wish_segment(text: str) -> bool:
    if not text:
        return False
    return bool(RE_WISH_AFTER.search(text) or RE_WISH_BEFORE.search(text))


def is_permanent_wish_pool(card: dict) -> bool:
    get_value = get_raw_get_field(card).strip()
    if not get_value:
        return False
    if get_value in {"å¸¸é©»", "è®¸æ„¿"}:
        return True
    return has_standalone_wish_segment(get_value)


def is_limited_wish_pool(card: dict) -> bool:
    permanent_flag = (card.get("permanent") or "").strip()
    if permanent_flag:
        return False
    raw_get = get_raw_get_field(card)
    return "é™æ—¶è®¸æ„¿" in raw_get


def ensure_pool_category_structure() -> dict:
    base = deepcopy(DEFAULT_POOL_CATEGORIES)
    for cat in base.values():
        if "subcategories" in cat:
            for sub in cat["subcategories"].values():
                sub["pools"] = []
        else:
            cat["pools"] = []
    return base


def get_category_priority(category_info: tuple[str, str | None]) -> int:
    return CATEGORY_PRIORITY.get(category_info, -1)


def classify_pool_category(card: dict) -> tuple[str, str | None]:
    if is_limited_wish_pool(card):
        return "wishSeries", "limited"
    if is_permanent_wish_pool(card):
        return "wishSeries", "permanent"
    return "specialRewards", None


def update_pool_categories_from_cards(cards: list[dict]) -> dict:
    def is_five_star(card_obj: dict) -> bool:
        star_value = (card_obj.get("star") or "").strip()
        return star_value.startswith("5")

    categories = ensure_pool_category_structure()
    pool_assignments: dict[str, tuple[str, str | None, int]] = {}
    pool_metadata: dict[str, dict] = {}

    for card in cards:
        if not is_five_star(card):
            continue
        raw_get_value = card.get("get", "")
        base_pool_name = extract_pool_name(raw_get_value)
        category_info = classify_pool_category(card)
        pool_name = base_pool_name
        if category_info == ("wishSeries", "limited"):
            quoted_name = extract_first_quote_name(raw_get_value)
            if quoted_name:
                pool_name = quoted_name
        elif category_info[0] == "specialRewards":
            pool_name = (card.get("name") or "").strip() or pool_name
        if not pool_name:
            continue

        meta = pool_metadata.setdefault(pool_name, {
            "characters": set(),
            "is_permanent": False,
        })
        character = (card.get("character") or "").strip()
        if character:
            meta["characters"].add(character)
        if category_info == ("wishSeries", "permanent"):
            meta["is_permanent"] = True

        priority = get_category_priority(category_info)
        previous = pool_assignments.get(pool_name)
        if previous and previous[2] >= priority:
            continue
        pool_assignments[pool_name] = (*category_info, priority)

    def build_pool_entry(name: str, meta: dict) -> dict:
        characters = meta.get("characters", set())
        role_count = len(characters)
        is_permanent_pool = meta.get("is_permanent", False)
        if is_permanent_pool:
            pool_type = "mixed"
        else:
            pool_type = "single" if role_count <= 1 else "mixed"
        return {
            "name": name,
            "poolType": pool_type,
            "roleCount": role_count,
            "isPermanent": is_permanent_pool,
        }

    for pool_name, info in pool_assignments.items():
        meta = pool_metadata.get(pool_name)
        if not meta:
            continue
        entry = build_pool_entry(pool_name, meta)
        cat_key, sub_key, _ = info
        if sub_key:
            categories[cat_key]["subcategories"].setdefault(sub_key, {"name": sub_key, "pools": []})
            categories[cat_key]["subcategories"][sub_key]["pools"].append(entry)
        else:
            categories[cat_key].setdefault("pools", [])
            categories[cat_key]["pools"].append(entry)

    with open(POOL_CATEGORIES_PATH, "w", encoding="utf-8") as fh:
        json.dump(categories, fh, ensure_ascii=False, indent=2)
    print(f"ğŸ¯ å·²æ›´æ–° {POOL_CATEGORIES_PATH}ï¼Œå…±å†™å…¥ {len(pool_assignments)} ä¸ªå¡æ± ã€‚", flush=True)

    return categories

# -----------------------------
# B ç«™åˆ† P æ”¯æŒ
# -----------------------------
video_information_cache = {}

def fetch_bilibili_video_info(bvid: str) -> dict:
    if not bvid:
        return {}
    if bvid in video_information_cache:
        return video_information_cache[bvid]
    api = f"https://api.bilibili.com/x/web-interface/view?bvid={bvid}"
    try:
        resp = polite_get(api)
        data = resp.json()
        video_information_cache[bvid] = data
        return data
    except Exception as e:
        print(f"âš ï¸ Bilibili API å¤±è´¥: {e}", flush=True)
        return {}

def find_dict_by_value(obj, target_value):
    if isinstance(obj, dict):
        for k, v in obj.items():
            if v == target_value:
                return obj
            got = find_dict_by_value(v, target_value)
            if got:
                return got
    elif isinstance(obj, list):
        for it in obj:
            got = find_dict_by_value(it, target_value)
            if got:
                return got
    return None

def build_player_url(src_tail_or_url: str, page_num: int | None) -> str:
    """
    æŠŠæŠ“åˆ°çš„ iframe src ç»Ÿä¸€è½¬æˆå¯å¤–é“¾çš„ bilibili æ’­æ”¾å™¨ URLï¼Œå¹¶è¡¥å¸¸ç”¨å‚æ•°ã€‚
    å…¼å®¹ä¼ å…¥æ˜¯ '.html?åé¢çš„æŸ¥è¯¢ä¸²'ï¼ˆå½¢å¦‚ 'bvid=...&p=...'ï¼‰ã€‚
    """
    url = src_tail_or_url
    if url.startswith(("aid=", "bvid=", "p=")):
        url = "https://player.bilibili.com/player.html?" + url
    if url.startswith("//"):
        url = "https:" + url

    p = urlparse(url)
    qs = parse_qs(p.query)
    if page_num is not None:
        qs["p"] = [str(page_num)]
    qs.setdefault("autoplay", ["auto"])
    qs.setdefault("preload", ["auto"])
    qs.setdefault("quality", ["1080p"])
    qs["isOutside"] = ["true"]

    new_q = urlencode({k: v[-1] for k, v in qs.items()})
    return urlunparse(p._replace(query=new_q))

# -----------------------------
# MediaWiki è¯»å–
# -----------------------------
_mw_site: mwclient.Site | None = None

def _get_mw_site(max_tries: int = 3) -> mwclient.Site | None:
    """
    åˆå§‹åŒ– mwclient.Siteï¼Œå¸¦æœ‰é™æ¬¡æ•°é‡è¯•ï¼Œé¿å…ç¬æ—¶ç½‘ç»œæŠ–åŠ¨å¯¼è‡´è„šæœ¬å´©æºƒã€‚
    """
    global _mw_site
    if _mw_site is not None:
        return _mw_site

    for attempt in range(1, max_tries + 1):
        try:
            site = mwclient.Site(host="wiki.biligame.com", path="/lysk/", clients_useragent=UA)
            _mw_site = site
            return site
        except Exception as exc:
            wait = 2 * attempt
            print(
                f"âš ï¸ mwclient åˆå§‹åŒ–å¤±è´¥ {attempt}/{max_tries}ï¼š{exc}ï¼Œ{wait}s åé‡è¯•",
                flush=True,
            )
            time.sleep(wait)
    return None

def wiki_detailed_info(card_name: str) -> dict:
    """
    ä½¿ç”¨ mwclient è¯»å–é¡µé¢æ–‡æœ¬å¹¶è§£æå­—æ®µã€‚
    å…¼å®¹æ¨¡æ¿è¡Œå‰å¯¼ '|'ï¼›å¤±è´¥è¿”å› {}ã€‚
    """
    global _mw_site
    site = _get_mw_site()
    if site is None:
        return {}
    field_map = {
        "æ€å¿µè§’è‰²": "character",
        "æ€å¿µåç§°": "name",
        "æ€å¿µä½ç½®": "card_type_tag",
        "æ€å¿µæ˜Ÿè°±": "card_color_tag",
        "æ€å¿µæ˜Ÿçº§": "star",
        "æ€å¿µå¤©èµ‹": "talent",
        "æ€å¿µè·å–é€”å¾„": "get",
        "å¸¸é©»": "permanent",
        "æ€å¿µä¸Šçº¿æ—¶é—´": "time",
    }
    max_tries = 5
    for i in range(1, max_tries + 1):
        try:
            page = site.pages[card_name]
            text = page.text() or ""
            info = {}
            for line in text.split("\n"):
                if "=" not in line:
                    continue
                k, v = line.split("=", 1)
                k = k.replace("|", "").strip()  # å…³é”®ï¼šå»æ‰ç®¡é“ç¬¦ä¸ç©ºæ ¼
                v = v.strip()
                if k in field_map:
                    if k == "æ€å¿µæ˜Ÿçº§" and v and not v.endswith("æ˜Ÿ"):
                        v += "æ˜Ÿ"
                    info[field_map[k]] = v
            return info
        except Exception as e:
            print(f"âš ï¸ mwclient è·å–å¤±è´¥ {card_name} ({i}/{max_tries}): {e}", flush=True)
            time.sleep(2 * i)
            _mw_site = None  # å¼ºåˆ¶ä¸‹ä¸€è½®é‡æ–°åˆå§‹åŒ–è¿æ¥
            site = _get_mw_site()
            if site is None:
                break
    return {}

def is_card_data_complete(card: dict) -> bool:
    required = ["character", "name", "star", "card_color_tag", "card_type_tag", "talent", "get", "time"]
    return all(card.get(f) for f in required)

# -----------------------------
# è¯¦æƒ…é¡µè§£æ
# -----------------------------
def fetch_detail_image(detail_url: str, card_name: str, max_retry: int = 3):
    """
    è¿”å›ï¼šsmall_img, big_img, video_url
    - ä¸ä¸‹è½½æ–‡ä»¶
    - å…¼å®¹ iframe è¢«è½¬ä¹‰æˆæ–‡æœ¬
    - å°½é‡åŒ¹é…åˆ† P
    - æ”¯æŒè‡ªåŠ¨é‡è¯•ï¼ˆé»˜è®¤æœ€å¤š 3 æ¬¡ï¼‰
    """
    small_img = big_img = video_url = ""

    for attempt in range(1, max_retry + 1):
        try:
            res = polite_get(detail_url)
            soup = BeautifulSoup(res.content, "html.parser")

            # å›¾ç‰‡ï¼šåªå¼•ç”¨é“¾æ¥ï¼Œä¸æ‹¼ç›´é“¾
            img = soup.select_one(".center img") or soup.select_one("img")
            if img:
                src = img.get("src", "") or ""
                srcset = img.get("srcset", "") or ""
                big_img = parse_best_from_srcset(srcset) or src
                small_img = src

            # è§†é¢‘ï¼šç”¨åŸå§‹ bytes/è½¬ä¹‰æ–‡æœ¬å…œåº•æŠ“ <iframe>
            blocks = extract_iframes(res.content)
            if blocks:
                m = IFRAME_SRC_RE.search(blocks[0])
                raw_src = m.group(1) if m else ""
                if raw_src:
                    # æå– bvid å¹¶å°½å¯èƒ½åŒ¹é…åˆ† P
                    bvid = ""
                    try:
                        q = parse_qs(urlparse(raw_src).query)
                        bvid = (q.get("bvid") or [""])[0]
                    except Exception:
                        pass

                    page_num = None
                    if bvid:
                        data = fetch_bilibili_video_info(bvid)
                        hit = find_dict_by_value(data, card_name)
                        if hit:
                            page_num = hit.get("page")

                    # å…¼å®¹ '.html?xxx' å°¾å·´
                    tail_or_url = raw_src.split(".html?")[-1] if ".html?" in raw_src else raw_src
                    video_url = build_player_url(tail_or_url, page_num)
                    print(video_url, flush=True)

            # æˆåŠŸåˆ™ç›´æ¥è¿”å›
            return small_img, big_img, video_url

        except Exception as e:
            print(f"âŒ ç¬¬ {attempt} æ¬¡è·å–è¯¦æƒ…é¡µå¤±è´¥ï¼š{detail_url}ï¼Œé”™è¯¯ï¼š{e}", flush=True)
            if attempt < max_retry:
                print(f"ğŸ” {1 if max_retry - attempt == 1 else max_retry - attempt} æ¬¡é‡è¯•å‰©ä½™ï¼Œç­‰å¾… 1 ç§’åé‡è¯•...", flush=True)
                time.sleep(1)
            else:
                print("ğŸš« å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒé‡è¯•ã€‚", flush=True)

    # å…¨éƒ¨å¤±è´¥åˆ™è¿”å›ç©ºç»“æœ
    return small_img, big_img, video_url

# -----------------------------
# ä¸»æµç¨‹
# -----------------------------
def main():
    print("ğŸš€ å¼€å§‹çˆ¬å–ï¼š", ENTRY_URL, flush=True)
    all_cards = []

    # å…¥å£é¡µ
    entry = polite_get(ENTRY_URL)
    soup = BeautifulSoup(entry.text, "html.parser")
    boxes = soup.select("div.divsort") or []
    print(f"å…±å‘ç°å¡ç‰‡ï¼š{len(boxes)}", flush=True)

    for idx, box in enumerate(boxes, 1):
        name_el = box.select_one(".card-name")
        if not name_el:
            continue
        card_name = name_el.text.strip()
        print(f"{idx}. {card_name}", flush=True)

        # åŸºç¡€ä¿¡æ¯ï¼ˆmwclientï¼‰
        info = wiki_detailed_info(card_name)

        # è¯¦æƒ…é“¾æ¥
        detail_a = box.select_one(".card-img a")
        detail_url = urljoin(WIKI_BASE, detail_a.get("href")) if (detail_a and detail_a.get("href")) else ""

        # åˆ—è¡¨ä¸­çš„å°å›¾ï¼ˆå…œåº•ï¼‰
        list_small = ""
        img_in_list = box.select_one("a.image img")
        if img_in_list:
            list_small = parse_best_from_srcset(img_in_list.get("srcset", "") or "") or img_in_list.get("src", "") or ""

        # è¯¦æƒ…é¡µå›¾ç‰‡/è§†é¢‘
        small_img = big_img = video_url = ""
        if detail_url:
            small_img, big_img, video_url = fetch_detail_image(detail_url, card_name)

        # å­—æ®µå®Œæ•´æ€§é‡è¯•ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰
        tries = 0
        while not is_card_data_complete(info) and tries < 3:
            print(f"  â†º å­—æ®µä¸å…¨ï¼Œé‡è¯• {tries+1}/3ï¼š{card_name}", flush=True)
            time.sleep(2)
            info = wiki_detailed_info(card_name)
            tries += 1

        # æœ€ç»ˆå›¾é“¾æ¥
        final_small = small_img or list_small or ""
        final_big = big_img or final_small

        # æ±‡æ€»
        info["image_small"] = final_small
        info["image"] = final_big
        info["video_url"] = video_url

        all_cards.append(info)


    # with Path(CARDS_PATH).open("r", encoding="utf-8") as fh:
    #     all_cards = json.load(fh)

    # å¡æ± åˆ†ç±»
    update_pool_categories_from_cards(all_cards)

    # ä¿å­˜
    with open(CARDS_PATH, "w", encoding="utf-8") as f:
        json.dump(all_cards, f, ensure_ascii=False, indent=2)

    print(f"âœ… å®Œæˆï¼Œå…±ä¿å­˜ {len(all_cards)} æ¡ï¼Œè¾“å‡ºï¼š{CARDS_PATH}", flush=True)

if __name__ == "__main__":
    main()
