# -*- coding: utf-8 -*-
"""
æ‹ä¸æ·±ç©º WIKI æŠ½å¡å¡ç‰‡çˆ¬è™«
----------------------------------------------------------------
- ä»…ä¿å­˜å›¾ç‰‡/è§†é¢‘é“¾æ¥ï¼Œä¸ä¸‹è½½æ–‡ä»¶
- ç¤¼è²Œ UAã€é™é€Ÿã€Session+Retry
- å…¼å®¹ iframe è¢«è½¬ä¹‰æˆæ–‡æœ¬ï¼ˆ&lt;iframe ...&gt;ï¼‰
- B ç«™æ’­æ”¾å™¨ URL è‡ªåŠ¨è¡¥å‚æ•°å¹¶å°è¯•åŒ¹é…åˆ† P
"""

import json
import time
import random
import re
import html
import requests
import mwclient
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# -----------------------------
# é…ç½®
# -----------------------------
CARDS_PATH = "src/assets/cards.json"
WIKI_BASE = "https://wiki.biligame.com/lysk/"
ENTRY_URL = WIKI_BASE + "%E6%80%9D%E5%BF%B5:%E7%AD%9B%E9%80%89"

# ç»Ÿä¸€è®¸å¯
LICENSE_NAME = "CC BY-NC-SA 4.0"
LICENSE_URL = "https://creativecommons.org/licenses/by-nc-sa/4.0/"

UA = "GachaSimCrawler/1.0 (+mailto:chenczn3528@gmail.com)"  # åˆè§„ UA
HEADERS = {"User-Agent": UA}

# -----------------------------
# ä¼šè¯ä¸ç¤¼è²Œè®¿é—®
# -----------------------------
session = requests.Session()
retries = Retry(
    total=5,
    backoff_factor=1.4,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods=["GET"],
    raise_on_status=False,
)
session.mount("https://", HTTPAdapter(max_retries=retries))
session.headers.update(HEADERS)

def polite_get(url: str, timeout: int = 15) -> requests.Response:
    """å¸¦éšæœºå»¶è¿Ÿçš„ GETï¼Œé¿å…ç»™ç«™ç‚¹é€ æˆå‹åŠ›ã€‚"""
    time.sleep(1.5 + random.random())  # 1.5~2.5s
    resp = session.get(url, timeout=timeout)
    resp.raise_for_status()
    if not resp.encoding:
        resp.encoding = resp.apparent_encoding or "utf-8"
    return resp

# -----------------------------
# å·¥å…·å‡½æ•°
# -----------------------------
def parse_best_from_srcset(srcset: str) -> str:
    """ä» srcset é€‰æ‹©æ¸…æ™°åº¦è¾ƒå¥½çš„ URLï¼šä¼˜å…ˆ 2xï¼Œæ²¡æœ‰å°±æœ€åä¸€é¡¹ã€‚"""
    if not srcset:
        return ""
    parts = [p.strip() for p in srcset.split(",") if p.strip()]
    if not parts:
        return ""
    for part in reversed(parts):
        if part.endswith(" 2x") or part.endswith("2x"):
            return part.rsplit(" ", 1)[0]
    return parts[-1].rsplit(" ", 1)[0] if " " in parts[-1] else parts[-1]

# â€”â€” ä»åŸå§‹ HTML ä¸­ç›´æ¥æŠ“ <iframe> å—ï¼›å¹¶å…¼å®¹è¢«è½¬ä¹‰çš„ HTML
IFRAME_BLOCK_RE = re.compile(r'<iframe.*?</iframe>', re.IGNORECASE | re.DOTALL)
IFRAME_SRC_RE   = re.compile(r'src=["\']([^"\']+)["\']', re.IGNORECASE)

def extract_iframes(html_bytes_or_text) -> list[str]:
    """
    ä»åŸå§‹ HTMLï¼ˆbytes æˆ– strï¼‰æå– <iframe> å—ã€‚
    1) ç›´æ¥æ­£åˆ™
    2) è‹¥é¡µé¢æ˜¯è½¬ä¹‰æ–‡æœ¬ï¼ˆ&lt;iframe ...&gt;ï¼‰ï¼Œå…ˆ html.unescape å†æ­£åˆ™
    """
    if isinstance(html_bytes_or_text, bytes):
        raw = html_bytes_or_text.decode("utf-8", errors="ignore")
    else:
        raw = html_bytes_or_text or ""

    blocks = IFRAME_BLOCK_RE.findall(raw)
    if blocks:
        return blocks

    unescaped = html.unescape(raw)
    if unescaped != raw:
        blocks = IFRAME_BLOCK_RE.findall(unescaped)
        if blocks:
            return blocks

    return []

# -----------------------------
# B ç«™åˆ† P æ”¯æŒ
# -----------------------------
video_information_cache = {}

def fetch_bilibili_video_info(bvid: str) -> dict:
    if not bvid:
        return {}
    if bvid in video_information_cache:
        return video_information_cache[bvid]
    api = f"https://api.bilibili.com/x/web-interface/view?bvid={bvid}"
    try:
        resp = polite_get(api)
        data = resp.json()
        video_information_cache[bvid] = data
        return data
    except Exception as e:
        print(f"âš ï¸ Bilibili API å¤±è´¥: {e}", flush=True)
        return {}

def find_dict_by_value(obj, target_value):
    if isinstance(obj, dict):
        for k, v in obj.items():
            if v == target_value:
                return obj
            got = find_dict_by_value(v, target_value)
            if got:
                return got
    elif isinstance(obj, list):
        for it in obj:
            got = find_dict_by_value(it, target_value)
            if got:
                return got
    return None

def build_player_url(src_tail_or_url: str, page_num: int | None) -> str:
    """
    æŠŠæŠ“åˆ°çš„ iframe src ç»Ÿä¸€è½¬æˆå¯å¤–é“¾çš„ bilibili æ’­æ”¾å™¨ URLï¼Œå¹¶è¡¥å¸¸ç”¨å‚æ•°ã€‚
    å…¼å®¹ä¼ å…¥æ˜¯ '.html?åé¢çš„æŸ¥è¯¢ä¸²'ï¼ˆå½¢å¦‚ 'bvid=...&p=...'ï¼‰ã€‚
    """
    url = src_tail_or_url
    if url.startswith(("aid=", "bvid=", "p=")):
        url = "https://player.bilibili.com/player.html?" + url
    if url.startswith("//"):
        url = "https:" + url

    p = urlparse(url)
    qs = parse_qs(p.query)
    if page_num is not None:
        qs["p"] = [str(page_num)]
    qs.setdefault("autoplay", ["auto"])
    qs.setdefault("preload", ["auto"])
    qs.setdefault("quality", ["1080p"])
    qs["isOutside"] = ["true"]

    new_q = urlencode({k: v[-1] for k, v in qs.items()})
    return urlunparse(p._replace(query=new_q))

# -----------------------------
# MediaWiki è¯»å–
# -----------------------------
_mw_site: mwclient.Site | None = None

def _get_mw_site(max_tries: int = 3) -> mwclient.Site | None:
    """
    åˆå§‹åŒ– mwclient.Siteï¼Œå¸¦æœ‰é™æ¬¡æ•°é‡è¯•ï¼Œé¿å…ç¬æ—¶ç½‘ç»œæŠ–åŠ¨å¯¼è‡´è„šæœ¬å´©æºƒã€‚
    """
    global _mw_site
    if _mw_site is not None:
        return _mw_site

    for attempt in range(1, max_tries + 1):
        try:
            site = mwclient.Site(host="wiki.biligame.com", path="/lysk/", clients_useragent=UA)
            _mw_site = site
            return site
        except Exception as exc:
            wait = 2 * attempt
            print(
                f"âš ï¸ mwclient åˆå§‹åŒ–å¤±è´¥ {attempt}/{max_tries}ï¼š{exc}ï¼Œ{wait}s åé‡è¯•",
                flush=True,
            )
            time.sleep(wait)
    return None

def wiki_detailed_info(card_name: str) -> dict:
    """
    ä½¿ç”¨ mwclient è¯»å–é¡µé¢æ–‡æœ¬å¹¶è§£æå­—æ®µã€‚
    å…¼å®¹æ¨¡æ¿è¡Œå‰å¯¼ '|'ï¼›å¤±è´¥è¿”å› {}ã€‚
    """
    global _mw_site
    site = _get_mw_site()
    if site is None:
        return {}
    field_map = {
        "æ€å¿µè§’è‰²": "character",
        "æ€å¿µåç§°": "name",
        "æ€å¿µä½ç½®": "card_type_tag",
        "æ€å¿µæ˜Ÿè°±": "card_color_tag",
        "æ€å¿µæ˜Ÿçº§": "star",
        "æ€å¿µå¤©èµ‹": "talent",
        "æ€å¿µè·å–é€”å¾„": "get",
        "å¸¸é©»": "permanent",
        "æ€å¿µä¸Šçº¿æ—¶é—´": "time",
    }
    max_tries = 5
    for i in range(1, max_tries + 1):
        try:
            page = site.pages[card_name]
            text = page.text() or ""
            info = {}
            for line in text.split("\n"):
                if "=" not in line:
                    continue
                k, v = line.split("=", 1)
                k = k.replace("|", "").strip()  # å…³é”®ï¼šå»æ‰ç®¡é“ç¬¦ä¸ç©ºæ ¼
                v = v.strip()
                if k in field_map:
                    if k == "æ€å¿µæ˜Ÿçº§" and v and not v.endswith("æ˜Ÿ"):
                        v += "æ˜Ÿ"
                    info[field_map[k]] = v
            return info
        except Exception as e:
            print(f"âš ï¸ mwclient è·å–å¤±è´¥ {card_name} ({i}/{max_tries}): {e}", flush=True)
            time.sleep(2 * i)
            _mw_site = None  # å¼ºåˆ¶ä¸‹ä¸€è½®é‡æ–°åˆå§‹åŒ–è¿æ¥
            site = _get_mw_site()
            if site is None:
                break
    return {}

def is_card_data_complete(card: dict) -> bool:
    required = ["character", "name", "star", "card_color_tag", "card_type_tag", "talent", "get", "time"]
    return all(card.get(f) for f in required)

# -----------------------------
# è¯¦æƒ…é¡µè§£æ
# -----------------------------
def fetch_detail_image(detail_url: str, card_name: str):
    """
    è¿”å›ï¼šsmall_img, big_img, video_url
    - ä¸ä¸‹è½½æ–‡ä»¶
    - å…¼å®¹ iframe è¢«è½¬ä¹‰æˆæ–‡æœ¬
    - å°½é‡åŒ¹é…åˆ† P
    """
    small_img = big_img = video_url = ""
    try:
        res = polite_get(detail_url)
        soup = BeautifulSoup(res.content, "html.parser")

        # å›¾ç‰‡ï¼šåªå¼•ç”¨é“¾æ¥ï¼Œä¸æ‹¼ç›´é“¾
        img = soup.select_one(".center img") or soup.select_one("img")
        if img:
            src = img.get("src", "") or ""
            srcset = img.get("srcset", "") or ""
            big_img = parse_best_from_srcset(srcset) or src
            small_img = src

        # è§†é¢‘ï¼šç”¨åŸå§‹ bytes/è½¬ä¹‰æ–‡æœ¬å…œåº•æŠ“ <iframe>
        blocks = extract_iframes(res.content)
        if blocks:
            m = IFRAME_SRC_RE.search(blocks[0])
            raw_src = m.group(1) if m else ""
            if raw_src:
                # æå– bvid å¹¶å°½å¯èƒ½åŒ¹é…åˆ† P
                bvid = ""
                try:
                    q = parse_qs(urlparse(raw_src).query)
                    bvid = (q.get("bvid") or [""])[0]
                except Exception:
                    pass

                page_num = None
                if bvid:
                    data = fetch_bilibili_video_info(bvid)
                    hit = find_dict_by_value(data, card_name)
                    if hit:
                        page_num = hit.get("page")

                # å…¼å®¹ '.html?xxx' å°¾å·´
                tail_or_url = raw_src.split(".html?")[-1] if ".html?" in raw_src else raw_src
                video_url = build_player_url(tail_or_url, page_num)
                print(video_url, flush=True)

    except Exception as e:
        print(f"âŒ è·å–è¯¦æƒ…é¡µå¤±è´¥ï¼š{detail_url}ï¼Œé”™è¯¯ï¼š{e}", flush=True)

    return small_img, big_img, video_url

# -----------------------------
# ä¸»æµç¨‹
# -----------------------------
def main():
    print("ğŸš€ å¼€å§‹çˆ¬å–ï¼š", ENTRY_URL, flush=True)
    all_cards = []

    # å…¥å£é¡µ
    entry = polite_get(ENTRY_URL)
    soup = BeautifulSoup(entry.text, "html.parser")
    boxes = soup.select("div.divsort") or []
    print(f"å…±å‘ç°å¡ç‰‡ï¼š{len(boxes)}", flush=True)

    for idx, box in enumerate(boxes, 1):
        name_el = box.select_one(".card-name")
        if not name_el:
            continue
        card_name = name_el.text.strip()
        print(f"{idx}. {card_name}", flush=True)

        # åŸºç¡€ä¿¡æ¯ï¼ˆmwclientï¼‰
        info = wiki_detailed_info(card_name)

        # è¯¦æƒ…é“¾æ¥
        detail_a = box.select_one(".card-img a")
        detail_url = urljoin(WIKI_BASE, detail_a.get("href")) if (detail_a and detail_a.get("href")) else ""

        # åˆ—è¡¨ä¸­çš„å°å›¾ï¼ˆå…œåº•ï¼‰
        list_small = ""
        img_in_list = box.select_one("a.image img")
        if img_in_list:
            list_small = parse_best_from_srcset(img_in_list.get("srcset", "") or "") or img_in_list.get("src", "") or ""

        # è¯¦æƒ…é¡µå›¾ç‰‡/è§†é¢‘
        small_img = big_img = video_url = ""
        if detail_url:
            small_img, big_img, video_url = fetch_detail_image(detail_url, card_name)

        # å­—æ®µå®Œæ•´æ€§é‡è¯•ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰
        tries = 0
        while not is_card_data_complete(info) and tries < 3:
            print(f"  â†º å­—æ®µä¸å…¨ï¼Œé‡è¯• {tries+1}/3ï¼š{card_name}", flush=True)
            time.sleep(2)
            info = wiki_detailed_info(card_name)
            tries += 1

        # æœ€ç»ˆå›¾é“¾æ¥
        final_small = small_img or list_small or ""
        final_big = big_img or final_small

        # æ±‡æ€»
        info["image_small"] = final_small
        info["image"] = final_big
        info["video_url"] = video_url

        all_cards.append(info)

    # ä¿å­˜
    with open(CARDS_PATH, "w", encoding="utf-8") as f:
        json.dump(all_cards, f, ensure_ascii=False, indent=2)

    print(f"âœ… å®Œæˆï¼Œå…±ä¿å­˜ {len(all_cards)} æ¡ï¼Œè¾“å‡ºï¼š{CARDS_PATH}", flush=True)

if __name__ == "__main__":
    main()
